## Semi-Autonomous Navigation via Local Vision Language Models (VLMs)

This repository contains the source code and documentation for a final year BSc Computer Science dissertation project at Coventry University. The project focuses on developing a semi-autonomous robot capable of interpreting natural-language voice commands and navigating dynamic environments using real-time obstacle avoidance and strategic planning powered by local VLMs.

### Project Overview

The system utilizes a SunFounder PiCar-X as the physical agent, interfaced with a Raspberry Pi 4. High-level strategic reasoning is offloaded to a local laptop running Ollama, which processes visual data to generate navigation logic.

#### Key Features 



#### Hardware Stack

- Robot: SunFounder PiCar-X with Robot HAT v4.

- Controller: Raspberry Pi 4.

- Inference Server (Laptop): * OS: Ubuntu.

    - CPU: Ryzen 3.

    - GPU: AMD Radeon.

    - RAM: 9.6 GB physical + 16 GB Swap.